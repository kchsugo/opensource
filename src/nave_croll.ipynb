{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 키워드를 입력하세요 : 환겨\n",
      "총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : 2\n",
      "\n",
      "크롤링 중...\n",
      "크롤링 완료\n",
      "데이터프레임 변환\n",
      "                             title  \\\n",
      "0             SK이노, 해양환경보호 업무협약 체결   \n",
      "1  NH농협생명, 환경경영 국제표준 ISO14001 인증획득   \n",
      "\n",
      "                                             url  \n",
      "0  http://www.fnnews.com/news/202112090952204076  \n",
      "1    https://www.sedaily.com/NewsView/22V7OI4GRH  \n",
      "엑셀 저장 완료 | 경로 : C:\\Users\\chan hyung Kim\\anaconda_project\\네이버뉴스_환겨_2021-12-09_16시17분.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'\n",
    "\n",
    "\n",
    "\n",
    "query = input('검색 키워드를 입력하세요 : ')\n",
    "news_num = int(input('총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : '))\n",
    "query = query.replace(' ', '+')\n",
    "\n",
    "\n",
    "news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "\n",
    "req = requests.get(news_url.format(query))\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "news_dict = {}\n",
    "idx = 0\n",
    "cur_page = 1\n",
    "\n",
    "print()\n",
    "print('크롤링 중...')\n",
    "\n",
    "while idx < news_num:\n",
    "### 네이버 뉴스 웹페이지 구성이 바뀌어 태그명, class 속성 값 등을 수정함(20210126) ###\n",
    "    \n",
    "    table = soup.find('ul',{'class' : 'list_news'})\n",
    "    li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "    area_list = [li.find('div', {'class' : 'news_area'}) for li in li_list]\n",
    "    a_list = [area.find('a', {'class' : 'news_tit'}) for area in area_list]\n",
    "    \n",
    "    for n in a_list[:min(len(a_list), news_num-idx)]:\n",
    "        news_dict[idx] = {'title' : n.get('title'),\n",
    "                          'url' : n.get('href') }\n",
    "        idx += 1\n",
    "\n",
    "    cur_page += 1\n",
    "\n",
    "    pages = soup.find('div', {'class' : 'sc_page_inner'})\n",
    "    next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
    "    \n",
    "    req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "print('크롤링 완료')\n",
    "\n",
    "print('데이터프레임 변환')\n",
    "news_df = DataFrame(news_dict).T\n",
    "print(news_df)\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "xlsx_file_name = '네이버뉴스_{}_{}.xlsx'.format(query, date)\n",
    "\n",
    "\n",
    "news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('엑셀 저장 완료 | 경로 : {}\\\\{}'.format(folder_path, xlsx_file_name))\n",
    "os.startfile(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
